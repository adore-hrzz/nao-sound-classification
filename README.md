nao-sound-classification
========================

Sound classification module, distinguishing between articulated speech and other sounds.


Installation instructions (naoqi v1.14)
-------------------------

Dependencies
------------

sudo apt-get install libsndfile1-dev

Initialize workspace. In empty folder

qibuild init

qitoolchain create pc ../../naoqi-skd-1.14.5-linux64/toolchain.xml

qitoolchain add-package -c pc XTRACT ../SoundClass_pc.tgz

qibuild configure -c pc
(qibuild make -c pc)

Open project (CMakeLists.txt) in QtCreator (v5.x.x), specify build folder corrrectly!

For building 3rd party packages

Get Opennao VM (for building 3rd party packages and running them on the robot)

Workflow
========

1) Configure (Configurator)

build-pc/sdk/bin/Configurator <Config_file_name>

configuration saved to SC.config (by default)

Configuration file is line-oriented
0: Sound library folder (where sound samples are located)
files must be named: #_klasa.wav
# - proizvoljan broj (redni broj snimanja)
klasa: ovime se zadaju klase
format zvuka je proizvoljan, tisina se automatski eliminira (pri snimanju); prilagodjeno inputu iz Listener aplikacija

1: Sound library list (nazivi snimljenih datoteka, bez wav ekstenzije; sluzi odabiru learning dataseta - ne mora se uciti iz cijelog dataseta)

2: Feature selection string (Learner i autoClassify ga citaju, svako slovo predstavlja 1 feature koji ce se koristiti za klasifikaciju, enkodirano u features.h)

3: Sound features data (csv table generated by learner)

- each row corrensponds to one chunk
- columns correspond to feature values (several columns can belog to one feature - some features are vectors)
- only features selected by the feature selection string are present; always ordered as in the feature selection string

4: Learned classes list

- machine learning model encodes classes as numbers; this file provides (generated by the learner) the mapping between the numbers and the class names (part of file names of the input sound files); class listed in the first row is encoded with 0, class in the second row is 1 etc 

5: Classifying model

- xml file generated by machine learning (Learner app);
- OpenCV file for encoding the machine learning model

6: Robot IP

- used by Listener (for recording training dataset) and autoClassify (for classifying the incoming data stream)

7: Robot port

- used by Listener and autoClassify

8: Sound sample length [pow(2, x)]

- each recording is chopped up into pieces consisting of  2^(sound sample length) samples 

9: subSample length [pow(2, x)]

- each piece is chopped up into smaller piecesconsisting of 2^(subSample length) samples; currently used only by HZCRR (high zero crossing rate ratio) 


2) Record (Listener)

- needs SCModule to be running (provides sound filtering)

- running SCModule remotely (on the PC)

./SCModule --pip 192.168.1.106 --pport 9559

./Listener <config_file_name>

records, stores files to "Sound library folder", automatically updates "Sound library list"

3) Learn (Learner) - gradient boosted trees

- koristi feature selection string; ako je ukljucena opcija, optimira (odbacuje "beskorisne" feature)

- processes recordings in chunks/subchunks 

./Learn <config_file_name>

- when the feature set is optimized, we should add another config line specifying which features are used

4) Classify (autoClassify)

- processes recordings in chunks/subchunks 

- needs SCModule to be running

./autoClassify dm.config

5) Classify from file (fileClassify)

- read audio file (currently hardcoded)

- classify audio file without SCModule & NAO

